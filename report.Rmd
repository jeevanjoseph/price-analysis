---
title: "Statistical Predictive Model for Automobile Pricing"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Abstract

In this paper, I study classified adverisement data to build reusable statistical
models for performing price prediction or valuation of automobiles. I use two 
different approaches in building the models. The first is a Multiple Linear Regression, 
which belongs to a class of models knowd as Generalized Linear Models (GLM/GLIM)
and is a common and popular choice for modelling linear relationships, when the 
model assumptions are met.The second approach is to use Gradient Boosting Machine(GBM).
GBM is a method that vastly improves on a simple decision tree, by building an 
ensemble of decision trees, where each tree attempts to correct the mispredictions
of all previous trees. The approaches are evalualted for thier efficiency on 
large datasets and accuracy.

I use a open dataset that is comprised of classified advertisements 
from the website craigslist.org obtained through web scraping. The data set is 
cleaned and filtered to limit the study to main stream consumer automobiles. 
I prepare the data for analysis, perform exploratory analysis, and finally build 
reusable predictive models using both MLR and GBM techniques and compare the results. 


## Introduction

The automotive segment is dominated by dealers and middlemen who often inflate 
pricing and make it difficult to understand the true market worth of an automobile. 
There are proprietary databases in the domain, however they use closed data 
sources and provide little transparency. These proprietary databases also provide
valuations to indivudials while using the same tools and data to power trade-in 
and auction platforms for dealers (Kelley Blue Book Inc.). 

The automotive industry's reliance on proprietary databases and the lack of transparency 
around the currently avaialable valuation models provide motivation 
to build an open data model for valuation. Such a model will be free from any 
potential conflicts of interests, and will be fully transparent.


## Research Question
Can we build a resusable statistical model to perform automobile valuations 
based purely on open data ?

When building a prediction or classification model, the prediction performance is 
the most important factor. I believe that the performance and effciency of training
a model is equally important. To this end I use two different approaches in builing
predictive models and compare those models. The two approaches used are Multiple
Linear Regression and Gradient Boosting Machine.

Multiple linear regression (MLR) is a parametric method that attempts to model 
the relationship between two or more explanatory variables and a response variable 
by fitting a linear equation to observed data. In the least-squares model, the 
best-fitting line for the observed data is calculated by minimizing the sum of 
the squares of the vertical deviations from each data  point to the line. 
MLR is a popular and classical method to model linear relationships,however it's 
effectiveness and applicability depends on the assumptions that it makes about 
the input dataset, like the predictors having strong linear realtionships
to the response variable, absense of multi-collinearity or auto-correlation in 
the data. Heteroscedasticity and the presense of influential outliers can also 
result in lesser predictive accuracy or biased predictions.

Gradient boosting is a non-parametric method that is based on the concept of a 
decision tree. Decision trees themselves generally have rather poor predictive 
power, and generally a tree with a very large number of splits is needed to reach 
an acceptable predictive accuracy. However they do have a number of desirable 
qualities like thier ability to handle large datasets, mixed predictors, missing
data, and redundant variable. The decision tree is improved by methods like boosting, 
where the approach is to fit many trees to re-weighted versions of the training 
data. Gradient boosting is an algorithm that works with a loss functions, including 
regression models. Gradient boosting builds decision trees from the training data 
drawing samples with replacement. Each subsequent tree that is built models the 
residuals of all the previous trees. This method keeps the favorable aspects of 
decision trees while it improves the predictive power of the model.

Both models are compared with the same dataset as well and the results are presented.

In this study, I use open market data obtained from the classifieds website
craigslist.org and build a prediction model based on this data. 

This study is aimed at creating a valuation model based on open-data from classified
advertisements. Studies have shown that users prefer third party price evaluations
than the dealershipâ€™s price evaluations and can have a positive impact on the 
sale itself (Cox Automotive Inc.). I believe this study provides a valuable vector 
in building an open and comprehensive model that makes the data transparent and 
free of any conflicts of interest.

### Setup

First we ensure that the required packages for our analysis are installed.
```{r}
#install.packages("tidyverse")
library(tidyr)
library(readr)
library(dplyr)
library(ggplot2)
source("ford.R", local = TRUE)
```


## Exploratory Data analysis

The first step in any investigation of data is to examine the univariate statistics
of the variables in the dataset. we load and summarize the data to develop a better 
understanding of the distribution and detect any anomalies such as extreme and 
abberant values. This also helps us get an idea of some orders of magnitude 
(such as the average ages, prices, mileage of the automobile listings) which will
be useful in the subsequent analysis.

### Loading the data

Now we load the data. The data load uses the `read_csv` function from the `dplyr` package.
```{R}
data <- read_csv("craigslistVehicles_semi.csv");
cities <- read_csv("cities.csv");
cities <- distinct(cities,city_url,.keep_all = TRUE)
modelTypes <- read_csv("model-types.csv")
```


Lets quickly summarise the data that was loaded. From the summary we can see that 
the data needs to be cleaned, as several features the set have missing values, and 
the univariate statistics for the numeric data shows the existence of extreme values.

```{R}
data$manufacturer <- factor(data$manufacturer)
data$condition <- factor(data$condition)
data$title_status <- factor(data$title_status)
data$type <- factor(data$type)
data$size <- factor(data$size)
data$fuel <- factor(data$fuel)
data$cylinders <- factor(data$cylinders)
data$transmission <- factor(data$transmission)
data$paint_color <- factor(data$paint_color)
data$drive <- factor(data$drive)
summary(data);
#plot(as.numeric(data$year),as.numeric(data$odometer))
#levels(data$type)
#summary(data$type)
#data %>%group_by(type)%>%summarise(count = n()) %>% View
```

### Remove incomplete observations from the dataset

We can see from the summary that we have observations in our dataset that are incomplete. 
We have missing and extreme values for several variables, and we shall remove 
these incomplete observations.
```{R}
complete <- data %>%  
    drop_na(odometer) %>%
    drop_na(year) %>%
    drop_na(cylinders) %>%
    drop_na(manufacturer) %>%
    drop_na(title_status) %>% 
    drop_na(condition) %>%
    drop_na(fuel) %>%
    drop_na(transmission) %>%
    drop_na(paint_color) %>%
    drop_na(drive) %>%
    drop_na(make)

```

#### Remove duplicate listings

A vehicle identification number (VIN) is a unique code, including a serial number, 
used by the automotive industry to identify individual motor vehicles, 
towed vehicles, motorcycles, scooters and mopeds, as defined in ISO 3779 
(content and structure) and ISO 4030 (location and attachment). (https://vpic.nhtsa.dot.gov/)

We can check for an remove duplicates by using the VIN number, in cases where it
is provided. We exclude the cases where VIN number is missing, as we cannot be 
sure if the classifieds are for the same automobile, so as to minimize autocollinearity
in the data we use for model building.

```{r}
no_vin <- complete %>% filter(is.na(VIN))

no_dupe_vin <- complete %>% filter(!is.na(VIN)) %>% distinct(VIN,.keep_all = TRUE)

valid_vin <- no_dupe_vin  %>% filter(str_length(stri_enc_toutf8(VIN))==17)

complete <- valid_vin
```

### Mainstream consumer automobiles

The automotive domain is varied and diverse from the type of vehicles to the how
pricing is calculated. There are several factors that make the valuation of an
automobile very specialized. Some of these factors are
- **Exotic Cars** - Exotic cars make a very small percentage of the market, but they are
  priced significantly differently than their mainstream counterparts.
- *Classic Cars* - Classic cars and Antique cars are also a specialized market, where
  the valuation primarily depends on the appraisals and heritage. The ownership costs
  of these vehicles are also significantly different. Most insurance companies 
  consider vehicles more than 18 years of age to be categorized as classic.
- *Custom Cars / Aftermarket customizations* - Custom built cars and heavy aftermarket
  customization are cars that are not generally available, or have been modified 
  significantly from their original specifiactions that they cannot be valuated 
  using the manufacturer's soecifications.

#### Rare and Exotic Automobiles
A glance at the distribution of the population among the various manufacturers 
indicate that some manufacturers have a very small presense in the market. 
We may have too few observations from these manufacturers, in other words, these
are rare values in the . Rare values can create bias in factor analysis and other 
analyses, by appearing to be more important than they really are (Tuffrey). 
```{R}
# clean manufacturers
manu_dist <- complete %>% group_by(manufacturer)%>%summarise(count = n()) %>% arrange(count)
manu_dist
manu_dist$manufacturer <- factor(manu_dist$manufacturer, levels = manu_dist$manufacturer[order(manu_dist$count)])
ggplot(manu_dist, aes(x=manufacturer, y=count, fill=manufacturer)) +
    geom_bar(stat="identity", width=1)
```

We exclude the brands whose inventory make up less than 2% of the total market from
the study, this excludes the exotic and rare automobiles.

```{R}
sig_manu <- complete %>% group_by(manufacturer)%>%summarise(count = n()) %>% filter(count>(.02*nrow(complete)))

manufacturer_filtered <- complete %>% filter(manufacturer %in% sig_manu$manufacturer)

manu_dist <- manufacturer_filtered %>% group_by(manufacturer)%>%summarise(count = n()) %>% arrange(count)
manu_dist$manufacturer <- factor(manu_dist$manufacturer, levels = manu_dist$manufacturer[order(manu_dist$count)])
ggplot(manu_dist, aes(x=manufacturer, y=count, fill=manufacturer)) +
    geom_bar(stat="identity", width=1)
```

#### Classic, Antique and Collector Cars

Classic, Antique and Collector Cars are valuated primarily 
by appraisal of their condition by a third party, and such data is effectively 
missing for us.
http://www.vmrintl.com/exotics/articles/about-exotic-cars.html 

For the study, I exclude the cars that fall in to this category by eliminating 
the vehicles that are more than 20 years old.
https://www.carinsurance.com/how-old-classic-car.aspx

This step also converts the categorial variable `Year` in to a continuous variable
`Age`. This removes the dependence of our model on the levels of the factor `Year`
and makes it resuable. We also eliminate invalid values from our sample that list 
vehicles that have a model year that has not been relesed yet (The most recent 
model avaialble in October 2019 is the 2020 model year)

```{R}
year_dist <- manufacturer_filtered %>% group_by(year) %>% summarise(count = n()) %>% arrange(count)
ggplot(data=year_dist,aes(x=year,y=count)) + geom_point()

manufacturer_filtered$age <- with(manufacturer_filtered, 2020-year)
age_filtered <- manufacturer_filtered %>% filter(age<=20 & age>=0)
```

The distribution of the new `age` varible is shown below.

```{r}
ggplot(data = age_filtered,aes(x=age))+geom_histogram( binwidth=1, fill="#69b3a2", color="#e9ecef", alpha=0.9)

```

### Understanding the Odometer distribution

Univariate statistics of the odometer variable indicates that have a single influential
outlier. This can be inferred from the fact that the third quantile and the Max 
are several orders of magnitude different.

```{R}
summary(age_filtered$odometer)

```

Further inspection by discretizing the range of the odometer readings in to 10 
equal intervals, we see that a single observarion is in the 10th interval and 
every other observation being in the 1st interval clearly indicating that the 
observationis anomalous. The value itself is 102,102,785 or 102 million miles.
This value is so extreme that we can assume that its erroneous and a data entry 
mistake - likely the user meant 102,785 and repeated the character sequence 102
```{R}
table(cut(age_filtered$odometer, breaks = 10))
```
From the data released by the Fedral Highway administration's office of 
Highway policy Information, vehicles travel an average of 11,789 miles /year.
https://www.fhwa.dot.gov/policyinformation/statistics/2017/vm1.cfm
https://nepis.epa.gov/Exe/ZyPDF.cgi?Dockey=P100U8YT.pdf
I am taking a more conservative estimate of 17,500 miles per year for a total of
vehicles with less than 350,000 miles on it.


I also exclude observations that have an odometer value of `0` unless its a new 
vehicle. In most cases its not reasonable for an automobile to have a `0` 
odometer value, unless its for a new vehicle and even then it would likely be 
rounded to zero, as miles are registered on the vehicle from the vehicle assmebly
line itself as part of the manufacturing process.
The extreme observations are removed.

```{R}
quantile(age_filtered$odometer,probs = seq(0,1,.01))

odo_filter_upper <- age_filtered %>% filter(odometer<=quantile(odometer,0.99))
odo_filtered <- odo_filter_upper %>% 
    filter(odometer >= quantile(odometer,0.02)  )
    


odo_dist<-odo_filtered %>%group_by(odometer) %>% summarise(count = n()) 
ggplot(data=odo_dist,aes(x=odometer,y=count)) + geom_point(aes(colour="#69b3a2"))
```

The distribution indicates that while some of the observations report very specific 
mileage, other approximate or round off the mileage to a nearest number.
this is a case where we can discretize the continuous variables to eliminate the
variance caused by the reporting. A quick histograpm shows us the frequency 
distribution of the variable values. We can see that automobiles with less than 150,000
miles on the odometer are most common and beyond that the number of automobiles 
on the market tapers off with increasing odometer readings. This pattern holds in 
line with conventional knowledge about the lifespan of an automobile.
```{R}
hist(odo_filtered$odometer,col = "112")
ggplot(data = odo_filtered,aes(x=odometer))+geom_histogram(binwidth=2500, fill="#69b3a2", color="#e9ecef", alpha=0.9)

```
As an initial strategy, I select the bin sizes to split the dataset in to 20 bins,
with approximately the same number of automobiles in every bin.

```{R}
odo_binned <- odo_filtered %>% mutate(mileage = cut(odometer, breaks = quantile(complete$odometer,probs = seq(0,1,.05)))) 
mileage_dist <- odo_binned %>% group_by(mileage) %>% summarise(count=n(),pr=median(price))
```

We can observe from the frequency distribution that as the odometer reading increases,
there are more cars being brought to the market. The shading also indicates an 
inverse relationship with between the odometer values and price.
```{R}
ggplot(data=mileage_dist,aes(x=mileage,y=count,fill=pr)) + geom_bar(stat="identity", width=1)
```


### Understanding the Price distribution
Understandig the Univariate and Multivariate statistics on the pricing hepls us 
build statistical inferece of the distribution of pricing and visualize simple 
hypothesis of association.

#### Univariate statistics

A quick glance at the univariate statistics of the price variable shows some 
patterns. The histogram for the price seems to have concentrated the values in 
to a single bucket. This is typically an idication that we may be dealing with 
extreme values in our dataset. Breaking the price down to percentiles, we can
clearly see that this is infact the case. The 99th and the 100th percentiles are 
several orders of magnitude different.


```{R}
# clean price

#univariate statistics
summary(odo_binned$price)
# hsitogram
hist(odo_binned$price)
quantile(odo_binned$price,probs = seq(0,1,.01))
```

I eliminate these outliers that make up the 100th percentile. Plotting the 
resulting distribution reveals more extreme values on the lower end.  

```{R}
price_dist<-odo_binned %>%
    filter(price<= quantile(complete$price,.99)) %>% 
    group_by(price) %>% 
    summarise(count = n()) 
ggplot(data=price_dist,aes(x=price,y=count)) + geom_point()
```

We can still see some of the extreme observations on te lower end. Specifically,
there are many observations that have a `price` of `0`. These typically mean that
the seller is trying to negotiate the best offer from potential buyers. We can 
see that upto the 10th percentile, the `price` is `0`. We are excluding these 
observations, as the price is not indicative of the actual expected sale price.

```{r}
price_filtered<-odo_binned %>% 
    filter(price > 0) %>% 
    filter(price >= quantile(odo_binned$price,.01) & price <= quantile(odo_binned$price,.99))
ggplot(data = price_filtered,aes(x=price))+geom_histogram( binwidth=1000, fill="#69b3a2", color="#e9ecef", alpha=0.9)
```

```{r}
price_dist<-price_filtered %>% 
    group_by(price) %>% 
    summarise(count = n()) 
ggplot(data=price_dist,aes(x=price,y=count)) + geom_point(aes(colour=price, size=count))
```

[ford]["https://www.ford.com"]
